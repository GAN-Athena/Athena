import torch
from torch.nn import *
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
from torchvision.transforms import *
import torchvision.utils as vutils
from torch.autograd import Variable
from Data.NeuralNetwork import *
import ctypes.wintypes
import os
import time
import datetime


def Train():
    # Setting some hyperparameters
    batchSize = 64  # We set the size of the batch.
    imageSize = 64  # We set the size of the generated images (64x64).

    # Creating the transformations
    transform = Compose([Resize(imageSize), ToTensor(),
                                    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])  # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.

    dataset = dset.CIFAR10(root='./Data', download=True,
                           transform=transform)  # We download the training set in the ./Data folder and we apply the previous transformations on each image.
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True,
                                             num_workers=0)  # We use dataLoader to get the images of the training set batch by batch.

    # Creating the generator
    netG = Generator()
    netG.apply(weights_init)
    # Creating the discriminator
    netD = Discriminator()
    netD.apply(weights_init)

    # Training the DCGANs
    criterion = BCELoss()
    optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))
    #Total Training will take an input from the user and set the total number of epochs to complete.
    Total_Training = 25
    #opening or creating the NeuralNetwork Loss log. If file exists, overwrites the file for a new session.
    open("NeuralNetwork Loss.txt", "w").close()

    for epoch in range(Total_Training):

        for i, data in enumerate(dataloader, 0):
            #initiating a timer to determine estimated completion time
            start = time.time()
            # 1st Step: Updating the weights of the neural network of the discriminator

            netD.zero_grad()

            # Training the discriminator with a real image of the dataset
            real, _ = data
            input = Variable(real)
            target = Variable(torch.ones(input.size()[0]))
            output = netD(input)
            errD_real = criterion(output, target)

            # Training the discriminator with a fake image generated by the generator
            noise = Variable(torch.randn(input.size()[0], 100, 1, 1))
            fake = netG(noise)
            target = Variable(torch.zeros(input.size()[0]))
            output = netD(fake.detach())
            errD_fake = criterion(output, target)

            # Backpropagating the total error
            errD = errD_real + errD_fake
            errD.backward()
            optimizerD.step()

            # 2nd Step: Updating the weights of the neural network of the generator

            netG.zero_grad()
            target = Variable(torch.ones(input.size()[0]))
            output = netD(fake)
            errG = criterion(output, target)
            errG.backward()
            optimizerG.step()

            # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps
            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, Total_Training, i, len(dataloader), errD.item(), errG.item()))
            if i % 100 == 0:

                if not os.path.exists('./results'):
                    os.makedirs('./results')
                vutils.save_image(real, '%s/real_samples.png' % "./results", normalize=True)
                fake = netG(noise)
                vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % ("./results", epoch), normalize=True)
            #ending the timer to create our estimated completion times
            end = time.time()
            #calculating the epoch trainging time
            epoch_time = len(dataloader) * (end-start)
            #printing the estimated time of completion for one epoch
            print("Estimated Time too Epoch Completion: {:0>8}".format(str(datetime.timedelta(seconds=epoch_time))))
            #estimating the completion time for all epochs entered by the user
            Total_Training_Time = epoch_time * Total_Training
            #printing the estimation to the screen for total training
            print("Estimated Time too Training Completion: {:0>8}".format(str(datetime.timedelta(seconds=Total_Training_Time))))
            #Appending the NeuralNetwork Loss Log
            file = open("NeuralNetwork Loss.txt", "a+")
            file.write('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, Total_Training, i, len(dataloader), errD.item(), errG.item()))
            file.write("\nEstimated Time too Epoch Completion: {:0>8}".format(str(datetime.timedelta(seconds=epoch_time))))
            file.write("\nEstimated Time too Training Completion: {:0>8}\n\n".format(str(datetime.timedelta(seconds=Total_Training_Time))))
            #Closing NeuralNetwork Loss Log
            file.close()


# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.
def weights_init(m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            m.weight.data.normal_(0.0, 0.02)
        elif classname.find('BatchNorm') != -1:
            m.weight.data.normal_(1.0, 0.02)
            m.bias.data.fill_(0)

# def loadImage():
#     # Setting some hyperparameters
#     batchSize = 3  # We set the size of the batch.
#     imageSize = 64  # We set the size of the generated images (64x64).
#
#     #Get User Document Folder
#     buf = ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH)
#     ctypes.windll.shell32.SHGetFolderPathW(None, 5, None, 0, buf)
#
#     try:
#         os.mkdir(buf.value + '\\Athena')
#     except OSError:
#         pass
#
#     return torch.utils.data.DataLoader(dset.ImageFolder(root=buf.value + '\\Athena',
#                                                         transform=Compose([Resize(imageSize),
#                                                                            ToTensor(),
#                                                                            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])),
#                                        batch_size=batchSize, shuffle=True, num_workers=0)
# def get_data(args, train_flag=True):
#     transform = Compose([
#         Scale(args.image_size),
#         CenterCrop(args.image_size),
#         ToTensor(),
#         Normalize(
#             (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
#     ])
#
#     if args.dataset in ['imagenet', 'folder', 'lfw']:
#         dataset = dset.ImageFolder(root=args.dataroot,
#                                    transform=transform)
#
#     elif args.dataset == 'lsun':
#         dataset = dset.LSUN(db_path=args.dataroot,
#                             classes=['bedroom_train'],
#                             transform=transform)
#
#     elif args.dataset == 'cifar10':
#         dataset = dset.CIFAR10(root=args.dataroot,
#                                download=True,
#                                train=train_flag,
#                                transform=transform)
#
#     elif args.dataset == 'cifar100':
#         dataset = dset.CIFAR100(root=args.dataroot,
#                                 download=True,
#                                 train=train_flag,
#                                 transform=transform)
#
#     elif args.dataset == 'mnist':
#         dataset = dset.MNIST(root=args.dataroot,
#                              download=True,
#                              train=train_flag,
#                              transform=transform)
#
#     else:
#         raise ValueError("Unknown dataset %s" % (args.dataset))
#     return dataset